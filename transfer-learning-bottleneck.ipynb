{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGDQYPL7KOF-",
        "colab_type": "code",
        "outputId": "5ca65f2f-4c39-48c5-cbd7-6bcb1d197a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense, Activation\n",
        "from keras import applications\n",
        "from keras import optimizers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "train_data_dir = 'dataset/train1'\n",
        "validation_data_dir = 'dataset/validation1'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-j97lI7Kago",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_train_samples = 1000\n",
        "nb_validation_samples = 200\n",
        "epochs = 50\n",
        "batch_size = 20\n",
        "\n",
        "def features():\n",
        "    img_width, img_height = 224, 224\n",
        "    \n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "    print('Model loaded.')\n",
        "\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    features_train = model.predict_generator(\n",
        "        train_generator, nb_train_samples//batch_size)\n",
        "    np.save(open('features_train.npy', 'wb'),\n",
        "            features_train)\n",
        "\n",
        "    val_generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    features_validation = model.predict_generator(\n",
        "        val_generator, nb_validation_samples//batch_size)\n",
        "    np.save(open('features_validation.npy', 'wb'),\n",
        "            features_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbanWYpBVCW3",
        "colab_type": "text"
      },
      "source": [
        "Since part of the rationale behind this experiment was to see how well the the pre-trained CNN does on a raw unprocessed dataset from a completely different domain than that it was trained on, data augmentation was performed minimally. It was to the extent of rescaling and resizing image size to 224 X 224, which is the default for the VGG16 model.\n",
        "\n",
        "The strategy followed for this problem was instantiating only the convolutional layers of the VGG16 model ie. upto the fully connected layers. After running the training and validation data once, the features from the last activation layers before the fully connected layers were recorded in 2 numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFctMscSKwtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = optimizers.SGD(lr=0.001, momentum=0.7)\n",
        "\n",
        "def train_model():\n",
        "    train_data = np.load(open('features_train.npy', 'rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * int((nb_train_samples / 2)) + [1] * int((nb_train_samples / 2)))\n",
        "\n",
        "    validation_data = np.load(open('features_validation.npy', 'rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * int((nb_validation_samples / 2)) + [1] * int((nb_validation_samples / 2)))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='sgd',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights('bottleneck_fc_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFJg0C6DVXW4",
        "colab_type": "text"
      },
      "source": [
        "A small fully-connected classifier model was consequently trained on top of the pre-trained models. The features stored after running the VGG16 model, were taken as inputs into the model. The weights were then saved in 'bottleneck_fc_model.h5', so we could later use them for fine-tuning.\n",
        "\n",
        "Dropout is implemented by only keeping a neuron active with some probability p(a hyperparameter), or setting it to zero otherwise. It can be interpreted as sampling a Neural Network within the full Neural Network, and only updating the parameters of the sampled network based on the input data. p=0.25 is a reasonable enough value, and so the dropout was kept at 0.25 in the model. Loss was computed through 'binary crossentropy' since the classification was carried out between 2 classes, and stochastic gradient descent with tunable parameters was chosen as the optimization function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18amOiwEK0LR",
        "colab_type": "code",
        "outputId": "ed56e0d9-c360-4627-8ce6-73a74555cfac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "features()\n",
        "train_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n",
            "Train on 1000 samples, validate on 200 samples\n",
            "Epoch 1/50\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.8349 - acc: 0.5220 - val_loss: 0.6793 - val_acc: 0.6200\n",
            "Epoch 2/50\n",
            "1000/1000 [==============================] - 1s 542us/step - loss: 0.6802 - acc: 0.5800 - val_loss: 0.6800 - val_acc: 0.5150\n",
            "Epoch 3/50\n",
            "1000/1000 [==============================] - 1s 575us/step - loss: 0.6702 - acc: 0.5780 - val_loss: 0.6525 - val_acc: 0.6500\n",
            "Epoch 4/50\n",
            "1000/1000 [==============================] - 0s 453us/step - loss: 0.6590 - acc: 0.6130 - val_loss: 0.6472 - val_acc: 0.6050\n",
            "Epoch 5/50\n",
            "1000/1000 [==============================] - 0s 458us/step - loss: 0.6467 - acc: 0.6230 - val_loss: 0.6346 - val_acc: 0.6450\n",
            "Epoch 6/50\n",
            "1000/1000 [==============================] - 1s 500us/step - loss: 0.6529 - acc: 0.6180 - val_loss: 0.6636 - val_acc: 0.5900\n",
            "Epoch 7/50\n",
            "1000/1000 [==============================] - 0s 469us/step - loss: 0.6266 - acc: 0.6540 - val_loss: 0.6346 - val_acc: 0.6200\n",
            "Epoch 8/50\n",
            "1000/1000 [==============================] - 0s 469us/step - loss: 0.5949 - acc: 0.6760 - val_loss: 0.6312 - val_acc: 0.6250\n",
            "Epoch 9/50\n",
            "1000/1000 [==============================] - 0s 483us/step - loss: 0.6034 - acc: 0.6840 - val_loss: 0.5767 - val_acc: 0.6850\n",
            "Epoch 10/50\n",
            "1000/1000 [==============================] - 0s 492us/step - loss: 0.5748 - acc: 0.7180 - val_loss: 0.5711 - val_acc: 0.6900\n",
            "Epoch 11/50\n",
            "1000/1000 [==============================] - 0s 473us/step - loss: 0.5805 - acc: 0.7030 - val_loss: 0.5460 - val_acc: 0.7250\n",
            "Epoch 12/50\n",
            "1000/1000 [==============================] - 0s 449us/step - loss: 0.5832 - acc: 0.7000 - val_loss: 0.6098 - val_acc: 0.6500\n",
            "Epoch 13/50\n",
            "1000/1000 [==============================] - 0s 461us/step - loss: 0.5691 - acc: 0.6990 - val_loss: 0.6976 - val_acc: 0.6100\n",
            "Epoch 14/50\n",
            "1000/1000 [==============================] - 0s 458us/step - loss: 0.5570 - acc: 0.7270 - val_loss: 0.5923 - val_acc: 0.6550\n",
            "Epoch 15/50\n",
            "1000/1000 [==============================] - 0s 476us/step - loss: 0.5407 - acc: 0.7250 - val_loss: 0.5601 - val_acc: 0.6950\n",
            "Epoch 16/50\n",
            "1000/1000 [==============================] - 0s 482us/step - loss: 0.5214 - acc: 0.7370 - val_loss: 0.5411 - val_acc: 0.7250\n",
            "Epoch 17/50\n",
            "1000/1000 [==============================] - 0s 461us/step - loss: 0.5278 - acc: 0.7310 - val_loss: 0.7931 - val_acc: 0.6000\n",
            "Epoch 18/50\n",
            "1000/1000 [==============================] - 0s 461us/step - loss: 0.5071 - acc: 0.7540 - val_loss: 0.6558 - val_acc: 0.6400\n",
            "Epoch 19/50\n",
            "1000/1000 [==============================] - 0s 470us/step - loss: 0.5163 - acc: 0.7390 - val_loss: 0.6753 - val_acc: 0.5850\n",
            "Epoch 20/50\n",
            "1000/1000 [==============================] - 0s 470us/step - loss: 0.4983 - acc: 0.7570 - val_loss: 0.5666 - val_acc: 0.7000\n",
            "Epoch 21/50\n",
            "1000/1000 [==============================] - 0s 467us/step - loss: 0.4869 - acc: 0.7590 - val_loss: 0.5464 - val_acc: 0.7100\n",
            "Epoch 22/50\n",
            "1000/1000 [==============================] - 0s 467us/step - loss: 0.4924 - acc: 0.7440 - val_loss: 0.5030 - val_acc: 0.7450\n",
            "Epoch 23/50\n",
            "1000/1000 [==============================] - 0s 454us/step - loss: 0.5219 - acc: 0.7470 - val_loss: 0.5525 - val_acc: 0.7650\n",
            "Epoch 24/50\n",
            "1000/1000 [==============================] - 0s 460us/step - loss: 0.5192 - acc: 0.7620 - val_loss: 0.5424 - val_acc: 0.7250\n",
            "Epoch 25/50\n",
            "1000/1000 [==============================] - 0s 445us/step - loss: 0.4781 - acc: 0.7750 - val_loss: 0.9958 - val_acc: 0.5350\n",
            "Epoch 26/50\n",
            "1000/1000 [==============================] - 0s 466us/step - loss: 0.5385 - acc: 0.7340 - val_loss: 0.5176 - val_acc: 0.7400\n",
            "Epoch 27/50\n",
            "1000/1000 [==============================] - 0s 486us/step - loss: 0.4552 - acc: 0.7920 - val_loss: 0.4877 - val_acc: 0.7800\n",
            "Epoch 28/50\n",
            "1000/1000 [==============================] - 0s 482us/step - loss: 0.5118 - acc: 0.7840 - val_loss: 0.5002 - val_acc: 0.7600\n",
            "Epoch 29/50\n",
            "1000/1000 [==============================] - 0s 473us/step - loss: 0.4725 - acc: 0.7640 - val_loss: 0.4834 - val_acc: 0.7750\n",
            "Epoch 30/50\n",
            "1000/1000 [==============================] - 0s 450us/step - loss: 0.5046 - acc: 0.7300 - val_loss: 0.4949 - val_acc: 0.7700\n",
            "Epoch 31/50\n",
            "1000/1000 [==============================] - 0s 465us/step - loss: 0.4568 - acc: 0.7820 - val_loss: 0.5254 - val_acc: 0.7450\n",
            "Epoch 32/50\n",
            "1000/1000 [==============================] - 0s 457us/step - loss: 0.4201 - acc: 0.8020 - val_loss: 0.5191 - val_acc: 0.7600\n",
            "Epoch 33/50\n",
            "1000/1000 [==============================] - 0s 473us/step - loss: 0.4447 - acc: 0.7950 - val_loss: 0.6043 - val_acc: 0.7100\n",
            "Epoch 34/50\n",
            "1000/1000 [==============================] - 0s 467us/step - loss: 0.4284 - acc: 0.7950 - val_loss: 0.5301 - val_acc: 0.7300\n",
            "Epoch 35/50\n",
            "1000/1000 [==============================] - 0s 484us/step - loss: 0.4374 - acc: 0.7840 - val_loss: 0.5135 - val_acc: 0.7400\n",
            "Epoch 36/50\n",
            "1000/1000 [==============================] - 0s 450us/step - loss: 0.4048 - acc: 0.8120 - val_loss: 0.5055 - val_acc: 0.7550\n",
            "Epoch 37/50\n",
            "1000/1000 [==============================] - 0s 446us/step - loss: 0.4208 - acc: 0.8060 - val_loss: 0.5521 - val_acc: 0.7300\n",
            "Epoch 38/50\n",
            "1000/1000 [==============================] - 0s 464us/step - loss: 0.3815 - acc: 0.8300 - val_loss: 0.5519 - val_acc: 0.7300\n",
            "Epoch 39/50\n",
            "1000/1000 [==============================] - 0s 441us/step - loss: 0.4138 - acc: 0.8050 - val_loss: 0.5511 - val_acc: 0.7400\n",
            "Epoch 40/50\n",
            "1000/1000 [==============================] - 0s 463us/step - loss: 0.4331 - acc: 0.8030 - val_loss: 0.4883 - val_acc: 0.7600\n",
            "Epoch 41/50\n",
            "1000/1000 [==============================] - 0s 452us/step - loss: 0.3530 - acc: 0.8480 - val_loss: 0.4864 - val_acc: 0.7550\n",
            "Epoch 42/50\n",
            "1000/1000 [==============================] - 0s 456us/step - loss: 0.3880 - acc: 0.8140 - val_loss: 0.7328 - val_acc: 0.6550\n",
            "Epoch 43/50\n",
            "1000/1000 [==============================] - 0s 441us/step - loss: 0.3780 - acc: 0.8210 - val_loss: 0.5940 - val_acc: 0.7200\n",
            "Epoch 44/50\n",
            "1000/1000 [==============================] - 0s 457us/step - loss: 0.3980 - acc: 0.8130 - val_loss: 0.4665 - val_acc: 0.7900\n",
            "Epoch 45/50\n",
            "1000/1000 [==============================] - 0s 440us/step - loss: 0.3444 - acc: 0.8570 - val_loss: 0.5893 - val_acc: 0.6950\n",
            "Epoch 46/50\n",
            "1000/1000 [==============================] - 0s 446us/step - loss: 0.3598 - acc: 0.8290 - val_loss: 0.4857 - val_acc: 0.7850\n",
            "Epoch 47/50\n",
            "1000/1000 [==============================] - 0s 466us/step - loss: 0.4012 - acc: 0.8240 - val_loss: 0.4666 - val_acc: 0.7900\n",
            "Epoch 48/50\n",
            "1000/1000 [==============================] - 0s 486us/step - loss: 0.3799 - acc: 0.8330 - val_loss: 0.4785 - val_acc: 0.7750\n",
            "Epoch 49/50\n",
            "1000/1000 [==============================] - 0s 456us/step - loss: 0.4096 - acc: 0.8090 - val_loss: 0.5164 - val_acc: 0.7650\n",
            "Epoch 50/50\n",
            "1000/1000 [==============================] - 0s 449us/step - loss: 0.3795 - acc: 0.8200 - val_loss: 0.4624 - val_acc: 0.7900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7pOc0OrV2ty",
        "colab_type": "text"
      },
      "source": [
        "Not bad! Using just the bottleneck features, the model reached a validation accuracy of 79% within 50 epochs (In this case, more number of epochs made it prone to overfit, so implemented Early Stopping). Of course, this performance is likely to improve if the model is finetuned after this point.\n"
      ]
    }
  ]
}